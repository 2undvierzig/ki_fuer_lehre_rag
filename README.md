# KI fÃ¼r Lehre - RAG System fÃ¼r Vorlesungsunterlagen

Ein vollstÃ¤ndiges Retrieval-Augmented Generation (RAG) System, das PDF-Vorlesungsunterlagen in eine durchsuchbare Wissensbasis konvertiert und Ã¼ber einen intelligenten Chat-Assistenten zugÃ¤nglich macht.

## ğŸ¯ Ãœberblick

Dieses System besteht aus zwei Hauptkomponenten:

1. **PDF zu Markdown Konverter**: Wandelt Ihre PDF-Dokumente in strukturierte Markdown-Dateien um
2. **RAG Terminal Chat**: Nutzt die konvertierten Dokumente als Wissensbasis fÃ¼r einen KI-Assistenten

### Workflow
```
PDF-Dateien â†’ Markdown-Konvertierung â†’ Vektor-Index â†’ Chat-Interface
    (data/)        (llm_ready/)         (storage/)      (Terminal)
```

## ğŸš€ Schnellstart

```bash
# 1. Umgebung vorbereiten
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# 2. AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# 3. Ollama installieren und starten
# Download: https://ollama.ai/download
ollama serve

# 4. BenÃ¶tigte Modelle herunterladen
ollama pull gemma3:27b
ollama pull nomic-embed-text

# 5. PDFs konvertieren
python convert_pdf.py

# 6. RAG Chat starten
python rag_terminal_chat.py
```

## ğŸ“‹ Systemanforderungen

- **Python**: 3.8 oder hÃ¶her
- **RAM**: 
  - Mindestens 8 GB (fÃ¼r kleine Modelle)
  - 16+ GB empfohlen (fÃ¼r optimale QualitÃ¤t mit gemma3:27b)
- **Speicherplatz**: 
  - ~4 GB fÃ¼r marker-pdf Modelle
  - ~16 GB fÃ¼r gemma3:27b
  - ~274 MB fÃ¼r nomic-embed-text
- **GPU**: Optional, aber empfohlen fÃ¼r schnellere Verarbeitung

## ğŸ“¦ Installation im Detail

### 1. Python-Umgebung

```bash
# Virtuelle Umgebung erstellen
python -m venv venv

# Aktivieren
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

### 2. AbhÃ¤ngigkeiten

```bash
# Alle Pakete installieren
pip install -r requirements.txt

# Wichtige Pakete:
# - marker-pdf==1.7.3 (PDF-Konvertierung)
# - llama-index>=0.12.0 (RAG-Framework)
# - ollama (LLM-Integration)
```

### 3. Ollama Setup

1. Ollama von https://ollama.ai/download herunterladen und installieren
2. Ollama-Service starten:
   ```bash
   ollama serve
   ```
3. Modelle installieren:
   ```bash
   # Hauptmodell (beste QualitÃ¤t, 16GB RAM)
   ollama pull gemma3:27b
   
   # Embedding-Modell (erforderlich)
   ollama pull nomic-embed-text
   
   # Alternative Modelle (weniger RAM):
   ollama pull gemma2:9b    # ~5.5GB RAM
   ollama pull gemma2:2b    # ~1.5GB RAM
   ```

## ğŸ“ Ordnerstruktur

```
KI fÃ¼r Lehre/
â”œâ”€â”€ data/                    # Eingabe: PDF-Dateien
â”‚   â””â”€â”€ *.pdf
â”œâ”€â”€ llm_ready/              # Konvertierte Markdown-Dateien
â”‚   â”œâ”€â”€ *.md
â”‚   â””â”€â”€ *_images/           # Extrahierte Bilder
â”œâ”€â”€ storage/                # Vektor-Index (automatisch erstellt)
â”œâ”€â”€ convert_pdf.py          # PDF-Konvertierungsskript
â”œâ”€â”€ rag_terminal_chat.py    # Chat-Anwendung
â”œâ”€â”€ rag_config.py          # Konfigurationsdatei
â”œâ”€â”€ requirements.txt        # Python-AbhÃ¤ngigkeiten
â””â”€â”€ README.md              # Diese Datei
```

## ğŸ”„ Schritt 1: PDF-Konvertierung

### Verwendung

1. **PDFs vorbereiten**: Alle PDF-Dateien in den `data/` Ordner legen

2. **Konvertierung starten**:
   ```bash
   python convert_pdf.py
   ```

3. **Ergebnisse**: Markdown-Dateien werden im `llm_ready/` Ordner erstellt

### Features

- âœ… Batch-Verarbeitung aller PDFs
- âœ… ErhÃ¤lt Dateinamen (nur .pdf â†’ .md)
- âœ… Extrahiert eingebettete Bilder
- âœ… UnterstÃ¼tzt komplexe Layouts und mathematische Formeln
- âœ… Fortschrittsanzeige wÃ¤hrend der Konvertierung

### Erweiterte Optionen

```bash
# OCR fÃ¼r alle Seiten erzwingen
OCR_ALL_PAGES=true python convert_pdf.py

# CPU statt GPU verwenden
TORCH_DEVICE=cpu python convert_pdf.py
```

## ğŸ’¬ Schritt 2: RAG Chat verwenden

### Grundlegende Nutzung

```bash
# Standard starten (gemma3:27b)
python rag_terminal_chat.py

# Mit alternativem Modell
python rag_terminal_chat.py --model gemma2:9b

# Index neu erstellen
python rag_terminal_chat.py --rebuild
```

### Chat-Befehle

- `/help` - Zeigt Hilfe und Tipps
- `/models` - Zeigt verfÃ¼gbare Modelle
- `/clear` - LÃ¶scht den Chat-Verlauf
- `/rebuild` - Erstellt den Index neu
- `/exit` - Beendet das Programm

### Beispiel-Interaktion

```
ğŸ‘¤ Du: Was ist Initialisierung in der Programmierung?
ğŸ¤– Assistent: Initialisierung bezeichnet den Vorgang, bei dem einer Variablen 
             ein Anfangswert zugewiesen wird...

ğŸ‘¤ Du: Gib mir ein Beispiel fÃ¼r Array-Initialisierung
ğŸ¤– Assistent: Hier sind einige Beispiele fÃ¼r Array-Initialisierung:
             int[] zahlen = {1, 2, 3, 4, 5};
             String[] namen = new String[10];
             ...
```

## âš™ï¸ Konfiguration

Bearbeiten Sie `rag_config.py` fÃ¼r erweiterte Einstellungen:

### Modellauswahl

```python
DEFAULT_CONFIG = {
    "llm_model": "gemma3:27b",  # Standard fÃ¼r beste QualitÃ¤t
    "embedding_model": "nomic-embed-text",
    "chunk_size": 512,
    "chunk_overlap": 50,
    "temperature": 0.7,
    ...Ã¼Ã¼p
}
```

### VerfÃ¼gbare Modelle

| Modell | RAM | Geschwindigkeit | QualitÃ¤t | Verwendung |
|--------|-----|-----------------|----------|------------|
| gemma2:2b | 4GB | â­â­â­â­â­ | â­â­â­ | Schnelle Tests |
| gemma2:9b | 8GB | â­â­â­â­ | â­â­â­â­ | Ausgewogen |
| gemma3:27b | 16GB+ | â­â­ | â­â­â­â­â­ | Beste QualitÃ¤t |

## ğŸ—ï¸ Technische Architektur

```
PDF-Konvertierung (marker-pdf)
â”œâ”€â”€ PDF Parser
â”œâ”€â”€ Layout-Analyse
â”œâ”€â”€ OCR (bei Bedarf)
â””â”€â”€ Markdown-Generator

RAG-System (LlamaIndex)
â”œâ”€â”€ Dokument-Loader
â”œâ”€â”€ Text-Splitter (Chunks)
â”œâ”€â”€ Embedding-Generator (nomic-embed-text)
â”œâ”€â”€ Vektor-Index
â”œâ”€â”€ Retriever (Top-K Similarity)
â”œâ”€â”€ LLM (Ollama)
â””â”€â”€ Chat-Memory
```

## ğŸ› Fehlerbehebung

### PDF-Konvertierung

**Problem**: "marker-pdf ist nicht installiert!"
```bash
pip install --upgrade marker-pdf==1.7.3
```

**Problem**: Speicherfehler bei groÃŸen PDFs
- Verarbeiten Sie groÃŸe PDFs einzeln
- Nutzen Sie `TORCH_DEVICE=cpu` fÃ¼r weniger RAM-Verbrauch

### RAG Chat

**Problem**: "Ollama Verbindung fehlgeschlagen"
```bash
# Ollama starten
ollama serve

# Verbindung testen
curl http://localhost:11434/api/tags
```

**Problem**: "Modell nicht gefunden"
```bash
# Modelle installieren
ollama pull gemma3:27b
ollama pull nomic-embed-text
```

**Problem**: Speicherfehler mit gemma3:27b
```bash
# Kleineres Modell verwenden
python rag_terminal_chat.py --model gemma2:9b
```

## ğŸ“Š Performance-Optimierung

### 1. Erste AusfÃ¼hrung
- PDF-Konvertierung: ~1-5 Minuten pro PDF
- Index-Erstellung: ~1-5 Minuten (einmalig)
- Modell-Download: ~10-30 Minuten (einmalig)

### 2. Folgende Nutzung
- Chat-Start: ~10-30 Sekunden
- Antwortzeit: ~2-10 Sekunden (je nach Modell)

### 3. Tipps
- Index wird gecacht in `./storage`
- Nutzen Sie `/rebuild` nur bei neuen Dokumenten
- Kleinere Chunk-GrÃ¶ÃŸen fÃ¼r prÃ¤zisere Antworten
- GrÃ¶ÃŸere Chunks fÃ¼r mehr Kontext

## ğŸ”§ Erweiterte Funktionen

### Batch-Verarbeitung von Fragen

```python
from rag_terminal_chat import RAGTerminalChat

# System initialisieren
rag = RAGTerminalChat()
rag.build_index()
rag.setup_chat_engine()

# Mehrere Fragen verarbeiten
fragen = [
    "Was ist Objektorientierung?",
    "ErklÃ¤re Vererbung",
    "Beispiel fÃ¼r Polymorphismus"
]

for frage in fragen:
    antwort = rag.chat_engine.chat(frage)
    print(f"F: {frage}\nA: {antwort}\n{'-'*50}\n")
```

### Eigene Modelle verwenden

1. Modell installieren:
   ```bash
   ollama pull ihr-modell:tag
   ```

2. In Konfiguration eintragen oder per CLI:
   ```bash
   python rag_terminal_chat.py --model ihr-modell:tag
   ```

## ğŸ“š Weitere Ressourcen

- [Ollama Dokumentation](https://github.com/ollama/ollama)
- [LlamaIndex Dokumentation](https://docs.llamaindex.ai/)
- [marker-pdf Repository](https://github.com/VikParuchuri/marker)
- [RAG Konzepte erklÃ¤rt](https://www.pinecone.io/learn/retrieval-augmented-generation/)

## ğŸ¤ Support

Bei Problemen prÃ¼fen Sie bitte:

- [ ] Python-Version (3.11+)
- [ ] Alle AbhÃ¤ngigkeiten installiert (`pip list`)
- [ ] Ollama lÃ¤uft (`ollama serve`)
- [ ] Modelle installiert (`ollama list`)
- [ ] PDF-Dateien im `data/` Ordner
- [ ] Konvertierte Dateien im `llm_ready/` Ordner
- [ ] Ausreichend RAM verfÃ¼gbar

---

Viel Erfolg beim Lernen mit Ihrem persÃ¶nlichen KI-Assistenten! ğŸ“ğŸ¤– 